import { PassThrough } from "stream";
import { EventEmitter } from "events";
import Utils from "./utils.js";
import Crypto from "crypto";
import retry from "async-retry";
import StreamToAsyncIterator from "./s2ai.js";
export class ChunkingUploader extends EventEmitter {
    tokenConfig;
    api;
    uploadID;
    token;
    chunkSize;
    batchSize;
    paused = false;
    isResume = false;
    uploadOptions;
    arbundles;
    constructor(tokenConfig, api) {
        super({ captureRejections: true });
        this.tokenConfig = tokenConfig;
        this.arbundles = this.tokenConfig.irys.arbundles;
        this.api = api;
        this.token = this.tokenConfig.name;
        this.chunkSize = 25_000_000;
        this.batchSize = 5;
        this.uploadID = "";
    }
    setResumeData(uploadID) {
        if (uploadID) {
            this.uploadID = uploadID;
            this.isResume = true;
        }
        return this;
    }
    /**
     * Note: Will return undefined unless an upload has been started.
     * @returns
     */
    getResumeData() {
        return this.uploadID;
    }
    setChunkSize(size) {
        if (size < 1) {
            throw new Error("Invalid chunk size (must be >=1)");
        }
        this.chunkSize = size;
        return this;
    }
    setBatchSize(size) {
        if (size < 1) {
            throw new Error("Invalid batch size (must be >=1)");
        }
        this.batchSize = size;
        return this;
    }
    pause() {
        this.emit("pause");
        this.paused = true;
    }
    resume() {
        this.paused = false;
        this.emit("resume");
    }
    async uploadTransaction(data, opts) {
        this.uploadOptions = opts;
        if (this.arbundles.DataItem.isDataItem(data)) {
            return this.runUpload(data.getRaw());
        }
        else {
            return this.runUpload(data);
        }
    }
    async uploadData(dataStream, options) {
        this.uploadOptions = options?.upload;
        return this.runUpload(dataStream, { ...options });
    }
    async runUpload(dataStream, transactionOpts) {
        let id = this.uploadID;
        const isTransaction = transactionOpts === undefined;
        const headers = { "x-chunking-version": "2" };
        let getres;
        if (!id) {
            getres = await this.api.get(`/chunks/${this.token}/-1/-1`, { headers });
            Utils.checkAndThrow(getres, "Getting upload token");
            this.uploadID = id = getres.data.id;
        }
        else {
            getres = await this.api.get(`/chunks/${this.token}/${id}/-1`, { headers });
            if (getres.status === 404)
                throw new Error(`Upload ID not found - your upload has probably expired.`);
            Utils.checkAndThrow(getres, "Getting upload info");
            if (this.chunkSize != +getres.data.size) {
                throw new Error(`Chunk size not equal to that of a previous upload (${+getres.data.size}).`);
            }
        }
        const { max, min } = getres.data;
        if (this.chunkSize < +min || this.chunkSize > +max) {
            throw new Error(`Chunk size out of allowed range: ${min} - ${max}`);
        }
        let totalUploaded = 0;
        const promiseFactory = (d, o, c) => {
            return new Promise((r) => {
                retry(async (bail) => {
                    await this.api
                        .post(`/chunks/${this.token}/${id}/${o}`, d, {
                        headers: { "Content-Type": "application/octet-stream", ...headers },
                        maxBodyLength: Infinity,
                        maxContentLength: Infinity,
                    })
                        .then((re) => {
                        if (re?.status >= 300) {
                            const e = { res: re, id: c, offset: o, size: d.length };
                            this.emit("chunkError", e);
                            if (re?.status === 402)
                                bail(new Error("Not enough funds to send data"));
                            throw e;
                        }
                        this.emit("chunkUpload", { id: c, offset: o, size: d.length, totalUploaded: (totalUploaded += d.length) });
                        r({ o, d: re });
                    });
                }),
                    { retries: 3, minTimeout: 1000, maxTimeout: 10_000 };
            });
        };
        const present = getres.data.chunks ?? [];
        const stream = new PassThrough();
        let cache = Buffer.alloc(0);
        let ended = false;
        let hasData = true;
        stream.on("end", () => (ended = true));
        stream.on("error", (e) => {
            throw new Error(`Error processing readable: ${e}`);
        });
        // custom as we need to read any number of bytes.
        const readBytes = async (size) => {
            while (!ended) {
                if (cache.length >= size) {
                    data = Buffer.from(cache.slice(0, size)); // force a copy
                    cache = cache.slice(size);
                    return data;
                }
                // eslint-disable-next-line no-var
                var data = stream.read(size);
                if (data === null) {
                    // wait for stream refill (perferred over setImmeadiate due to multi env support)
                    await new Promise((r) => setTimeout((r) => r(true), 0, r));
                    continue;
                }
                if (data.length === size)
                    return data;
                cache = Buffer.concat([cache, data]);
            }
            // flush
            while (cache.length >= size) {
                data = Buffer.from(cache.slice(0, size)); // force a copy
                cache = cache.slice(size);
                return data;
            }
            hasData = false;
            return cache;
        };
        let tx;
        let txHeaderLength;
        // doesn't matter if we randomise ID (anchor) between resumes, as the tx header/signing info is always uploaded last.
        if (!isTransaction) {
            tx = this.arbundles.createData("", this.tokenConfig.getSigner(), {
                ...transactionOpts,
                anchor: transactionOpts?.anchor ?? Crypto.randomBytes(32).toString("base64").slice(0, 32),
            });
            const raw = tx.getRaw();
            txHeaderLength = raw.length;
            stream.write(raw);
            totalUploaded -= raw.length;
        }
        if (Buffer.isBuffer(dataStream)) {
            stream.write(dataStream);
            stream.end();
        }
        else if ("pipe" in dataStream) {
            dataStream.pipe(stream);
        }
        else {
            throw new Error("Input data is not a buffer or a compatible stream (no .pipe method)");
        }
        let offset = 0;
        const processing = new Set();
        let chunkID = 0;
        let heldChunk;
        let teeStream;
        let deephash;
        if (!isTransaction) {
            teeStream = new PassThrough();
            const txLength = tx.getRaw().length;
            if (this.chunkSize < txHeaderLength)
                throw new Error(`Configured chunk size is too small for transaction header! (${this.chunkSize} < ${txHeaderLength})`);
            heldChunk = await readBytes(this.chunkSize);
            chunkID++;
            offset += heldChunk.length;
            teeStream.write(heldChunk.slice(txLength));
            const sigComponents = [
                this.arbundles.stringToBuffer("dataitem"),
                this.arbundles.stringToBuffer("1"),
                this.arbundles.stringToBuffer(tx.signatureType.toString()),
                tx.rawOwner,
                tx.rawTarget,
                tx.rawAnchor,
                tx.rawTags,
                new StreamToAsyncIterator(teeStream),
            ];
            // do *not* await, this needs to process in parallel to the upload process.
            deephash = this.arbundles.deepHash(sigComponents);
        }
        let nextPresent = present.pop();
        // Consume data while there's data to read.
        while (hasData) {
            if (this.paused) {
                await new Promise((r) => this.on("resume", () => r(undefined)));
            }
            // do not upload data that's already present
            if (nextPresent) {
                const delta = +nextPresent[0] - offset;
                if (delta <= this.chunkSize) {
                    const bytesToSkip = nextPresent[1];
                    const data = await readBytes(bytesToSkip);
                    if (!isTransaction)
                        teeStream.write(data);
                    offset += bytesToSkip;
                    nextPresent = present.pop();
                    chunkID++;
                    totalUploaded += bytesToSkip;
                    continue;
                }
            }
            const chunk = await readBytes(this.chunkSize);
            if (!isTransaction)
                teeStream.write(chunk);
            while (processing.size >= this.batchSize) {
                // get & then remove resolved promise from processing set
                const [p] = await Promise.race(processing);
                processing.delete(p);
            }
            // self-referencing promise
            const promise = (async () => await promiseFactory(chunk, offset, ++chunkID))().then((value) => [promise, value]);
            processing.add(promise);
            offset += chunk.length;
        }
        if (teeStream)
            teeStream.end();
        await Promise.all(processing);
        if (!isTransaction) {
            const hash = await deephash;
            const sigBytes = Buffer.from(await this.tokenConfig.getSigner().sign(hash));
            heldChunk.set(sigBytes, 2); // tx will be the first part of the held chunk.
            await promiseFactory(heldChunk, 0, 0);
        }
        // potential improvement: write chunks into a file at offsets, instead of individual chunks + doing a concatenating copy
        const finishUpload = await this.api.post(`/chunks/${this.token}/${id}/-1`, null, {
            headers: { "Content-Type": "application/octet-stream", ...headers },
            timeout: this.api.config?.timeout ?? 40_000 * 10, // server side reconstruction can take a while
        });
        if (finishUpload.status === 402) {
            throw new Error("Not enough funds to send data");
        }
        // this will throw if the dataItem reconstruction fails
        Utils.checkAndThrow(finishUpload, "Finalising upload", [201]);
        // Recover ID
        if (finishUpload.status === 201) {
            throw new Error(finishUpload.data);
        }
        finishUpload.data.verify = Utils.verifyReceipt.bind({}, this.arbundles, finishUpload.data.data);
        this.emit("done", finishUpload);
        return finishUpload;
    }
    get completionPromise() {
        return new Promise((r) => this.on("done", r));
    }
}
//# sourceMappingURL=chunkingUploader.js.map