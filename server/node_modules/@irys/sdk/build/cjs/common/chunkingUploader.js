"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.ChunkingUploader = void 0;
const stream_1 = require("stream");
const events_1 = require("events");
const utils_1 = __importDefault(require("./utils"));
const crypto_1 = __importDefault(require("crypto"));
const async_retry_1 = __importDefault(require("async-retry"));
const s2ai_1 = __importDefault(require("./s2ai"));
class ChunkingUploader extends events_1.EventEmitter {
    constructor(tokenConfig, api) {
        super({ captureRejections: true });
        this.paused = false;
        this.isResume = false;
        this.tokenConfig = tokenConfig;
        this.arbundles = this.tokenConfig.irys.arbundles;
        this.api = api;
        this.token = this.tokenConfig.name;
        this.chunkSize = 25000000;
        this.batchSize = 5;
        this.uploadID = "";
    }
    setResumeData(uploadID) {
        if (uploadID) {
            this.uploadID = uploadID;
            this.isResume = true;
        }
        return this;
    }
    /**
     * Note: Will return undefined unless an upload has been started.
     * @returns
     */
    getResumeData() {
        return this.uploadID;
    }
    setChunkSize(size) {
        if (size < 1) {
            throw new Error("Invalid chunk size (must be >=1)");
        }
        this.chunkSize = size;
        return this;
    }
    setBatchSize(size) {
        if (size < 1) {
            throw new Error("Invalid batch size (must be >=1)");
        }
        this.batchSize = size;
        return this;
    }
    pause() {
        this.emit("pause");
        this.paused = true;
    }
    resume() {
        this.paused = false;
        this.emit("resume");
    }
    uploadTransaction(data, opts) {
        return __awaiter(this, void 0, void 0, function* () {
            this.uploadOptions = opts;
            if (this.arbundles.DataItem.isDataItem(data)) {
                return this.runUpload(data.getRaw());
            }
            else {
                return this.runUpload(data);
            }
        });
    }
    uploadData(dataStream, options) {
        return __awaiter(this, void 0, void 0, function* () {
            this.uploadOptions = options === null || options === void 0 ? void 0 : options.upload;
            return this.runUpload(dataStream, Object.assign({}, options));
        });
    }
    runUpload(dataStream, transactionOpts) {
        var _a, _b, _c, _d;
        return __awaiter(this, void 0, void 0, function* () {
            let id = this.uploadID;
            const isTransaction = transactionOpts === undefined;
            const headers = { "x-chunking-version": "2" };
            let getres;
            if (!id) {
                getres = yield this.api.get(`/chunks/${this.token}/-1/-1`, { headers });
                utils_1.default.checkAndThrow(getres, "Getting upload token");
                this.uploadID = id = getres.data.id;
            }
            else {
                getres = yield this.api.get(`/chunks/${this.token}/${id}/-1`, { headers });
                if (getres.status === 404)
                    throw new Error(`Upload ID not found - your upload has probably expired.`);
                utils_1.default.checkAndThrow(getres, "Getting upload info");
                if (this.chunkSize != +getres.data.size) {
                    throw new Error(`Chunk size not equal to that of a previous upload (${+getres.data.size}).`);
                }
            }
            const { max, min } = getres.data;
            if (this.chunkSize < +min || this.chunkSize > +max) {
                throw new Error(`Chunk size out of allowed range: ${min} - ${max}`);
            }
            let totalUploaded = 0;
            const promiseFactory = (d, o, c) => {
                return new Promise((r) => {
                    (0, async_retry_1.default)((bail) => __awaiter(this, void 0, void 0, function* () {
                        yield this.api
                            .post(`/chunks/${this.token}/${id}/${o}`, d, {
                            headers: Object.assign({ "Content-Type": "application/octet-stream" }, headers),
                            maxBodyLength: Infinity,
                            maxContentLength: Infinity,
                        })
                            .then((re) => {
                            if ((re === null || re === void 0 ? void 0 : re.status) >= 300) {
                                const e = { res: re, id: c, offset: o, size: d.length };
                                this.emit("chunkError", e);
                                if ((re === null || re === void 0 ? void 0 : re.status) === 402)
                                    bail(new Error("Not enough funds to send data"));
                                throw e;
                            }
                            this.emit("chunkUpload", { id: c, offset: o, size: d.length, totalUploaded: (totalUploaded += d.length) });
                            r({ o, d: re });
                        });
                    })),
                        { retries: 3, minTimeout: 1000, maxTimeout: 10000 };
                });
            };
            const present = (_a = getres.data.chunks) !== null && _a !== void 0 ? _a : [];
            const stream = new stream_1.PassThrough();
            let cache = Buffer.alloc(0);
            let ended = false;
            let hasData = true;
            stream.on("end", () => (ended = true));
            stream.on("error", (e) => {
                throw new Error(`Error processing readable: ${e}`);
            });
            // custom as we need to read any number of bytes.
            const readBytes = (size) => __awaiter(this, void 0, void 0, function* () {
                while (!ended) {
                    if (cache.length >= size) {
                        data = Buffer.from(cache.slice(0, size)); // force a copy
                        cache = cache.slice(size);
                        return data;
                    }
                    // eslint-disable-next-line no-var
                    var data = stream.read(size);
                    if (data === null) {
                        // wait for stream refill (perferred over setImmeadiate due to multi env support)
                        yield new Promise((r) => setTimeout((r) => r(true), 0, r));
                        continue;
                    }
                    if (data.length === size)
                        return data;
                    cache = Buffer.concat([cache, data]);
                }
                // flush
                while (cache.length >= size) {
                    data = Buffer.from(cache.slice(0, size)); // force a copy
                    cache = cache.slice(size);
                    return data;
                }
                hasData = false;
                return cache;
            });
            let tx;
            let txHeaderLength;
            // doesn't matter if we randomise ID (anchor) between resumes, as the tx header/signing info is always uploaded last.
            if (!isTransaction) {
                tx = this.arbundles.createData("", this.tokenConfig.getSigner(), Object.assign(Object.assign({}, transactionOpts), { anchor: (_b = transactionOpts === null || transactionOpts === void 0 ? void 0 : transactionOpts.anchor) !== null && _b !== void 0 ? _b : crypto_1.default.randomBytes(32).toString("base64").slice(0, 32) }));
                const raw = tx.getRaw();
                txHeaderLength = raw.length;
                stream.write(raw);
                totalUploaded -= raw.length;
            }
            if (Buffer.isBuffer(dataStream)) {
                stream.write(dataStream);
                stream.end();
            }
            else if ("pipe" in dataStream) {
                dataStream.pipe(stream);
            }
            else {
                throw new Error("Input data is not a buffer or a compatible stream (no .pipe method)");
            }
            let offset = 0;
            const processing = new Set();
            let chunkID = 0;
            let heldChunk;
            let teeStream;
            let deephash;
            if (!isTransaction) {
                teeStream = new stream_1.PassThrough();
                const txLength = tx.getRaw().length;
                if (this.chunkSize < txHeaderLength)
                    throw new Error(`Configured chunk size is too small for transaction header! (${this.chunkSize} < ${txHeaderLength})`);
                heldChunk = yield readBytes(this.chunkSize);
                chunkID++;
                offset += heldChunk.length;
                teeStream.write(heldChunk.slice(txLength));
                const sigComponents = [
                    this.arbundles.stringToBuffer("dataitem"),
                    this.arbundles.stringToBuffer("1"),
                    this.arbundles.stringToBuffer(tx.signatureType.toString()),
                    tx.rawOwner,
                    tx.rawTarget,
                    tx.rawAnchor,
                    tx.rawTags,
                    new s2ai_1.default(teeStream),
                ];
                // do *not* await, this needs to process in parallel to the upload process.
                deephash = this.arbundles.deepHash(sigComponents);
            }
            let nextPresent = present.pop();
            // Consume data while there's data to read.
            while (hasData) {
                if (this.paused) {
                    yield new Promise((r) => this.on("resume", () => r(undefined)));
                }
                // do not upload data that's already present
                if (nextPresent) {
                    const delta = +nextPresent[0] - offset;
                    if (delta <= this.chunkSize) {
                        const bytesToSkip = nextPresent[1];
                        const data = yield readBytes(bytesToSkip);
                        if (!isTransaction)
                            teeStream.write(data);
                        offset += bytesToSkip;
                        nextPresent = present.pop();
                        chunkID++;
                        totalUploaded += bytesToSkip;
                        continue;
                    }
                }
                const chunk = yield readBytes(this.chunkSize);
                if (!isTransaction)
                    teeStream.write(chunk);
                while (processing.size >= this.batchSize) {
                    // get & then remove resolved promise from processing set
                    const [p] = yield Promise.race(processing);
                    processing.delete(p);
                }
                // self-referencing promise
                const promise = (() => __awaiter(this, void 0, void 0, function* () { return yield promiseFactory(chunk, offset, ++chunkID); }))().then((value) => [promise, value]);
                processing.add(promise);
                offset += chunk.length;
            }
            if (teeStream)
                teeStream.end();
            yield Promise.all(processing);
            if (!isTransaction) {
                const hash = yield deephash;
                const sigBytes = Buffer.from(yield this.tokenConfig.getSigner().sign(hash));
                heldChunk.set(sigBytes, 2); // tx will be the first part of the held chunk.
                yield promiseFactory(heldChunk, 0, 0);
            }
            // potential improvement: write chunks into a file at offsets, instead of individual chunks + doing a concatenating copy
            const finishUpload = yield this.api.post(`/chunks/${this.token}/${id}/-1`, null, {
                headers: Object.assign({ "Content-Type": "application/octet-stream" }, headers),
                timeout: (_d = (_c = this.api.config) === null || _c === void 0 ? void 0 : _c.timeout) !== null && _d !== void 0 ? _d : 40000 * 10, // server side reconstruction can take a while
            });
            if (finishUpload.status === 402) {
                throw new Error("Not enough funds to send data");
            }
            // this will throw if the dataItem reconstruction fails
            utils_1.default.checkAndThrow(finishUpload, "Finalising upload", [201]);
            // Recover ID
            if (finishUpload.status === 201) {
                throw new Error(finishUpload.data);
            }
            finishUpload.data.verify = utils_1.default.verifyReceipt.bind({}, this.arbundles, finishUpload.data.data);
            this.emit("done", finishUpload);
            return finishUpload;
        });
    }
    get completionPromise() {
        return new Promise((r) => this.on("done", r));
    }
}
exports.ChunkingUploader = ChunkingUploader;
//# sourceMappingURL=chunkingUploader.js.map